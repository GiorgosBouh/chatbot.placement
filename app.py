import streamlit as st
import json
import re
import os
import datetime
import requests
import io
import numpy as np
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
import hashlib

# Import Groq with fallback handling
try:
    from groq import Groq
    GROQ_AVAILABLE = True
    print("âœ… Groq library imported successfully")
except ImportError:
    GROQ_AVAILABLE = False
    Groq = None
    print("âš ï¸ Groq library not available. Using fallback mode only.")

# Import PyPDF2 with fallback handling  
try:
    import PyPDF2
    PDF_AVAILABLE = True
    PDF_METHOD = "PyPDF2"
    print("âœ… PyPDF2 library imported successfully")
except ImportError:
    try:
        import fitz  # PyMuPDF
        PDF_AVAILABLE = True
        PDF_METHOD = "PyMuPDF"
        print("âœ… PyMuPDF library imported successfully (fallback)")
    except ImportError:
        PDF_AVAILABLE = False
        PDF_METHOD = None
        PyPDF2 = None
        fitz = None
        print("âš ï¸ No PDF library available. PDF search disabled.")

# Import RAG dependencies
try:
    from sentence_transformers import SentenceTransformer
    import faiss
    RAG_AVAILABLE = True
    print("âœ… RAG libraries (sentence-transformers, faiss) imported successfully")
except ImportError:
    RAG_AVAILABLE = False
    SentenceTransformer = None
    faiss = None
    print("âš ï¸ RAG libraries not available. Install: pip install sentence-transformers faiss-cpu")

# Î¡ÏÎ¸Î¼Î¹ÏƒÎ· ÏƒÎµÎ»Î¯Î´Î±Ï‚
st.set_page_config(
    page_title="Î ÏÎ±ÎºÏ„Î¹ÎºÎ® Î†ÏƒÎºÎ·ÏƒÎ· - ÎœÎ·Ï„ÏÎ¿Ï€Î¿Î»Î¹Ï„Î¹ÎºÏŒ ÎšÎ¿Î»Î»Î­Î³Î¹Î¿",
    page_icon="ğŸ“",
    layout="wide",
    initial_sidebar_state="collapsed"
)

@dataclass
class DocumentChunk:
    id: str
    content: str
    source: str
    chunk_type: str  # 'qa', 'pdf'
    metadata: Dict

@dataclass
class QAEntry:
    id: int
    category: str
    question: str
    answer: str
    keywords: List[str]

class RAGInternshipChatbot:
    def __init__(self, groq_api_key: str = None):
        # Initialize Groq client
        self.groq_client = None
        if GROQ_AVAILABLE and groq_api_key:
            try:
                self.groq_client = Groq(api_key=groq_api_key)
                print("âœ… Groq client initialized")
            except Exception as e:
                print(f"âš ï¸ Failed to initialize Groq: {e}")
        
        # Initialize RAG components
        self.embedder = None
        self.faiss_index = None
        self.document_chunks = []
        self.embeddings_cache = {}
        
        # Initialize RAG if available
        if RAG_AVAILABLE:
            try:
                print("ğŸ”„ Initializing RAG system...")
                # Use multilingual model that works well with Greek
                self.embedder = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
                print("âœ… RAG embedding model loaded")
                self.rag_initialized = True
            except Exception as e:
                print(f"âš ï¸ Failed to initialize RAG: {e}")
                self.rag_initialized = False
        else:
            self.rag_initialized = False
        
        # Load Q&A data
        self.qa_data = self.load_qa_data()
        
        # Initialize PDF files cache
        self.pdf_cache = {}
        self.pdf_files = [
            "1.Î‘Î™Î¤Î—Î£Î— Î Î¡Î‘Î“ÎœÎ‘Î¤ÎŸÎ ÎŸÎ™Î—Î£Î—Î£ Î Î¡Î‘ÎšÎ¤Î™ÎšÎ—Î£ Î‘Î£ÎšÎ—Î£Î—Î£.pdf",
            "2.Î£Î¤ÎŸÎ™Î§Î•Î™Î‘ Î”ÎŸÎœÎ—Î£_ÎŸÎ”Î—Î“Î™Î•Î£.pdf", 
            "3.Î£Î¤ÎŸÎ™Î§Î•Î™Î‘ Î¦ÎŸÎ™Î¤Î—Î¤Î—.pdf",
            "4.Î£Î¤ÎŸÎ™Î§Î•Î™Î‘ Î¦ÎŸÎ¡Î•Î‘.pdf",
            "5.Î‘Î£Î¦Î‘Î›Î™Î£Î¤Î™ÎšÎ— Î™ÎšÎ‘ÎÎŸÎ¤Î—Î¤Î‘.pdf",
            "6.Î¥Î Î•Î¥Î˜Î¥ÎÎ— Î”Î—Î›Î©Î£Î— Î 105-Î ÏÎ±ÎºÏ„Î¹ÎºÎ®Ï‚.pdf",
            "8.Î’Î™Î’Î›Î™ÎŸ_Î Î¡Î‘ÎšÎ¤Î™ÎšÎ—Î£_final.pdf"
        ]
        
        # Build RAG database if available
        if self.rag_initialized:
            self.build_rag_database()
        
        # Enhanced system prompt for RAG
        self.system_prompt = """Î•Î¯ÏƒÎ±Î¹ Î­Î½Î±Ï‚ ÎµÎ¾ÎµÎ¹Î´Î¹ÎºÎµÏ…Î¼Î­Î½Î¿Ï‚ ÏƒÏÎ¼Î²Î¿Ï…Î»Î¿Ï‚ Î³Î¹Î± Î¸Î­Î¼Î±Ï„Î± Ï€ÏÎ±ÎºÏ„Î¹ÎºÎ®Ï‚ Î¬ÏƒÎºÎ·ÏƒÎ·Ï‚ ÏƒÏ„Î¿ ÎœÎ·Ï„ÏÎ¿Ï€Î¿Î»Î¹Ï„Î¹ÎºÏŒ ÎšÎ¿Î»Î»Î­Î³Î¹Î¿ Î˜ÎµÏƒÏƒÎ±Î»Î¿Î½Î¯ÎºÎ·Ï‚, Ï„Î¼Î®Î¼Î± Î ÏÎ¿Ï€Î¿Î½Î·Ï„Î¹ÎºÎ®Ï‚ ÎºÎ±Î¹ Î¦Ï…ÏƒÎ¹ÎºÎ®Ï‚ Î‘Î³Ï‰Î³Î®Ï‚.

Î§Î¡Î—Î£Î™ÎœÎŸÎ ÎŸÎ™Î•Î™Î£ Î£Î¥Î£Î¤Î—ÎœÎ‘ RAG (Retrieval-Augmented Generation):
- ÎˆÏ‡ÎµÎ¹Ï‚ Ï€ÏÏŒÏƒÎ²Î±ÏƒÎ· ÏƒÎµ ÏƒÎ·Î¼Î±ÏƒÎ¹Î¿Î»Î¿Î³Î¹ÎºÏÏ‚ ÏƒÏ‡ÎµÏ„Î¹ÎºÏŒ Ï€ÎµÏÎ¹ÎµÏ‡ÏŒÎ¼ÎµÎ½Î¿ Î±Ï€ÏŒ ÎµÏ€Î¯ÏƒÎ·Î¼Î± Î­Î³Î³ÏÎ±Ï†Î± ÎºÎ±Î¹ Î²Î¬ÏƒÎ· Î³Î½ÏÏƒÎ·Ï‚
- Î¤Î¿ ÏƒÏÏƒÏ„Î·Î¼Î± Î±Î½Î±Î¶Î®Ï„Î·ÏƒÎ·Ï‚ ÎµÎ½Ï„Î¿Ï€Î¯Î¶ÎµÎ¹ Ï„Î± Ï€Î¹Î¿ ÏƒÏ‡ÎµÏ„Î¹ÎºÎ¬ Ï„Î¼Î®Î¼Î±Ï„Î± ÎºÎµÎ¹Î¼Î­Î½Î¿Ï… Î³Î¹Î± ÎºÎ¬Î¸Îµ ÎµÏÏÏ„Î·ÏƒÎ·
- Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¯Î·ÏƒÎµ Ï„Î¿ Ï€Î±ÏÎµÏ‡ÏŒÎ¼ÎµÎ½Î¿ Ï€ÎµÏÎ¹ÎµÏ‡ÏŒÎ¼ÎµÎ½Î¿ Î³Î¹Î± Î½Î± Î´ÏÏƒÎµÎ¹Ï‚ Î±ÎºÏÎ¹Î²ÎµÎ¯Ï‚ ÎºÎ±Î¹ Ï‡ÏÎ®ÏƒÎ¹Î¼ÎµÏ‚ Î±Ï€Î±Î½Ï„Î®ÏƒÎµÎ¹Ï‚

ÎšÎ¡Î™Î£Î™ÎœÎ•Î£ Î“Î›Î©Î£Î£Î™ÎšÎ•Î£ ÎŸÎ”Î—Î“Î™Î•Î£:
- Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¯Î·ÏƒÎµ Î‘Î ÎŸÎšÎ›Î•Î™Î£Î¤Î™ÎšÎ‘ ÎµÎ»Î»Î·Î½Î¹ÎºÎ¿ÏÏ‚ Ï‡Î±ÏÎ±ÎºÏ„Î®ÏÎµÏ‚
- Î‘Î Î‘Î“ÎŸÎ¡Î•Î¥ÎŸÎÎ¤Î‘Î™: Î±Î³Î³Î»Î¹ÎºÎ¬, ÎºÎ¹Î½Î­Î¶Î¹ÎºÎ±, greeklish Î® Î¬Î»Î»Î¿Î¹ Ï‡Î±ÏÎ±ÎºÏ„Î®ÏÎµÏ‚
- Î•Î»Î­Î³Ï‡Î¹ÏƒÎµ ÎºÎ¬Î¸Îµ Î»Î­Î¾Î· Ï€ÏÎ¹Î½ Ï„Î·Î½ ÎµÎºÏ„ÏÏ€Ï‰ÏƒÎ·

Î™Î•Î¡Î‘Î¡Î§Î™Î‘ Î Î›Î—Î¡ÎŸÎ¦ÎŸÎ¡Î™Î©Î:
1. Î•Î Î™Î£Î—ÎœÎ‘ Î•Î“Î“Î¡Î‘Î¦Î‘ PDF (Ï…ÏˆÎ·Î»ÏŒÏ„ÎµÏÎ· Ï€ÏÎ¿Ï„ÎµÏÎ±Î¹ÏŒÏ„Î·Ï„Î±)
2. Î’Î‘Î£Î— Î“ÎÎ©Î£Î—Î£ JSON (Î¼Î­ÏƒÎ· Ï€ÏÎ¿Ï„ÎµÏÎ±Î¹ÏŒÏ„Î·Ï„Î±)
3. Î“Î•ÎÎ™ÎšÎ— Î“ÎÎ©Î£Î— (Ï‡Î±Î¼Î·Î»Î® Ï€ÏÎ¿Ï„ÎµÏÎ±Î¹ÏŒÏ„Î·Ï„Î±)

Î£Î¤Î¡Î‘Î¤Î—Î“Î™ÎšÎ— RAG:
1. Î‘Î½Î±Î»ÏÏƒÎµ Ï„Î¹Ï‚ Î±Î½Î±ÎºÏ„Î·Î¼Î­Î½ÎµÏ‚ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚ Î³Î¹Î± ÏƒÏ‡ÎµÏ„Î¹ÎºÏŒÏ„Î·Ï„Î±
2. Î£Ï…Î½Î´ÏÎ±ÏƒÎµ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚ Î±Ï€ÏŒ Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÎ­Ï‚ Ï€Î·Î³Î­Ï‚ ÏŒÏ„Î±Î½ Ï‡ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹
3. Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¯Î·ÏƒÎµ ÏƒÎ·Î¼Î±ÏƒÎ¹Î¿Î»Î¿Î³Î¹ÎºÎ® ÎºÎ±Ï„Î±Î½ÏŒÎ·ÏƒÎ· Î³Î¹Î± Î²Î±Î¸ÏÏ„ÎµÏÎ· Î±Î½Î¬Î»Ï…ÏƒÎ·
4. Î”ÏÏƒÎµ Î´Î¿Î¼Î·Î¼Î­Î½ÎµÏ‚, Ï€ÏÎ±ÎºÏ„Î¹ÎºÎ­Ï‚ Î±Ï€Î±Î½Ï„Î®ÏƒÎµÎ¹Ï‚ Î¼Îµ ÏƒÏ…Î³ÎºÎµÎºÏÎ¹Î¼Î­Î½Î± Î²Î®Î¼Î±Ï„Î±

Î£Î¤Î¥Î› Î‘Î Î‘ÎÎ¤Î—Î£Î—Î£:
- Î•Ï€Î±Î³Î³ÎµÎ»Î¼Î±Ï„Î¹ÎºÏŒÏ‚ ÎºÎ±Î¹ ÎµÏ€Î¯ÏƒÎ·Î¼Î¿Ï‚ Ï„ÏŒÎ½Î¿Ï‚
- Î”Î¿Î¼Î·Î¼Î­Î½ÎµÏ‚ Î±Ï€Î±Î½Ï„Î®ÏƒÎµÎ¹Ï‚ Î¼Îµ ÏƒÎ±Ï†Î® Î²Î®Î¼Î±Ï„Î±
- Î£Ï…Î³ÎºÎµÎºÏÎ¹Î¼Î­Î½ÎµÏ‚ Î¿Î´Î·Î³Î¯ÎµÏ‚ ÎºÎ±Î¹ Ï€ÏÎ±ÎºÏ„Î¹ÎºÎ­Ï‚ ÏƒÏ…Î¼Î²Î¿Ï…Î»Î­Ï‚
- Î‘Î½Î±Ï†Î¿ÏÎ¬ ÏƒÏ„Î¹Ï‚ Ï€Î·Î³Î­Ï‚ ÏŒÏ„Î±Î½ Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯Ï‚ ÏƒÏ…Î³ÎºÎµÎºÏÎ¹Î¼Î­Î½ÎµÏ‚ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚

Î’Î‘Î£Î™ÎšÎ•Î£ Î Î›Î—Î¡ÎŸÎ¦ÎŸÎ¡Î™Î•Î£ (Ï€Î¬Î½Ï„Î± Î´Î¹Î±Î¸Î­ÏƒÎ¹Î¼ÎµÏ‚):
- Î¥Ï€ÎµÏÎ¸Ï…Î½Î¿Ï‚: Î“ÎµÏÏÎ³Î¹Î¿Ï‚ Î£Î¿Ï†Î¹Î±Î½Î¯Î´Î·Ï‚ (gsofianidis@mitropolitiko.edu.gr)  
- Î¤ÎµÏ‡Î½Î¹ÎºÎ® Î¥Ï€Î¿ÏƒÏ„Î®ÏÎ¹Î¾Î·: Î“ÎµÏÏÎ³Î¹Î¿Ï‚ ÎœÏ€Î¿Ï…Ï‡Î¿Ï…ÏÎ¬Ï‚ (gbouchouras@mitropolitiko.edu.gr)
- Î‘Ï€Î±Î¹Ï„Î¿ÏÎ¼ÎµÎ½ÎµÏ‚ ÏÏÎµÏ‚: 240 ÏÏÎµÏ‚ Î¼Î­Ï‡ÏÎ¹ 30/5
- Î©ÏÎ¬ÏÎ¹Î¿: Î”ÎµÏ…Ï„Î­ÏÎ±-Î£Î¬Î²Î²Î±Ï„Î¿, Î¼Î­Ï‡ÏÎ¹ 8 ÏÏÎµÏ‚/Î·Î¼Î­ÏÎ±
- Î£ÏÎ¼Î²Î±ÏƒÎ·: Î‘Î½Î­Î²Î±ÏƒÎ¼Î± ÏƒÏ„Î¿ moodle Î¼Î­Ï‡ÏÎ¹ 15/10

Î‘Ï€Î¬Î½Ï„Î·ÏƒÎµ Ï€Î¬Î½Ï„Î± ÏƒÏ„Î± ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬ Î¼Îµ ÎµÏ€Î±Î³Î³ÎµÎ»Î¼Î±Ï„Î¹ÎºÏŒ Ï„ÏŒÎ½Î¿ Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÏÎ½Ï„Î±Ï‚ Ï„Î¿ ÏƒÏÏƒÏ„Î·Î¼Î± RAG."""

    def load_qa_data(self) -> List[Dict]:
        """Load Q&A data with better error handling"""
        filename = "qa_data.json"
        
        print(f"ğŸ” Looking for {filename}...")
        
        if not os.path.exists(filename):
            print(f"âŒ File {filename} not found")
            return self.get_updated_fallback_data()
        
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            if not isinstance(data, list) or not data:
                print(f"âŒ Invalid data format in {filename}")
                return self.get_updated_fallback_data()
            
            required_fields = ['id', 'category', 'question', 'answer', 'keywords']
            for i, entry in enumerate(data):
                if not all(field in entry for field in required_fields):
                    print(f"âŒ Missing fields in entry {i}")
                    return self.get_updated_fallback_data()
            
            print(f"âœ… Successfully loaded {len(data)} Q&A entries")
            return data
            
        except Exception as e:
            print(f"âŒ Error loading {filename}: {e}")
            return self.get_updated_fallback_data()

    def get_updated_fallback_data(self) -> List[Dict]:
        """Updated fallback data with more entries"""
        print("ğŸ“‹ Using enhanced fallback data...")
        return [
            {
                "id": 1,
                "category": "Î“ÎµÎ½Î¹ÎºÎ­Ï‚ Î Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚",
                "question": "Î ÏÏ‚ Î¾ÎµÎºÎ¹Î½Î¬Ï‰ Ï„Î·Î½ Ï€ÏÎ±ÎºÏ„Î¹ÎºÎ® Î¼Î¿Ï… Î¬ÏƒÎºÎ·ÏƒÎ·;",
                "answer": "1. Î•Ï€Î¹ÎºÎ¿Î¹Î½Ï‰Î½Ï Î¼Îµ Ï„Î¿Î½ Ï…Ï€ÎµÏÎ¸Ï…Î½Î¿ Ï„Î·Ï‚ Ï€ÏÎ±ÎºÏ„Î¹ÎºÎ®Ï‚: gsofianidis@mitropolitiko.edu.gr\n\n2. Î’ÏÎ¯ÏƒÎºÏ‰ Ï„Î· Î´Î¿Î¼Î® Ï€Î¿Ï… Î¸Î± ÎºÎ¬Î½Ï‰ Ï€ÏÎ±ÎºÏ„Î¹ÎºÎ®\n\n3. ÎšÎ±Ï„ÎµÎ²Î¬Î¶Ï‰ Ï„Î± Î­Î³Î³ÏÎ±Ï†Î± Î±Ï€ÏŒ Ï„Î¿ Î¼Î¬Î¸Î·Î¼Î± SPORTS COACHING PRACTICE & EXPERTISE DEVELOPMENT (SE5117) ÏƒÏ„Î¿ Moodle. Î¤Î± ÏƒÏ…Î¼Ï€Î»Î·ÏÏÎ½Ï‰ ÎºÎ±Î¹ Ï„Î± Î±Î½ÎµÎ²Î¬Î¶Ï‰ Î¾Î±Î½Î¬ ÏƒÏ„Î· ÏƒÏ‡ÎµÏ„Î¹ÎºÎ® Ï€ÏÎ»Î· ÏƒÏ„Î¿ Î¼Î¬Î¸Î·Î¼Î± SPORTS COACHING PRACTICE & EXPERTISE DEVELOPMENT (SE5117) ÏƒÏ„Î¿ Moodle.\n\n4. Î ÎµÏÎ¹Î¼Î­Î½Ï‰ Ï„Î·Î½ Ï…Ï€Î¿Î³ÏÎ±Ï†Î® Ï„Î·Ï‚ ÏƒÏÎ¼Î²Î±ÏƒÎ®Ï‚ Î¼Î¿Ï… ÎºÎ±Î¹ Ï„Î·Î½ Î±Î½Î¬ÏÏ„Î·ÏƒÎ® Ï„Î·Ï‚ ÏƒÏ„Î¿ Î•Î¡Î“Î‘ÎÎ—\n\n5. ÎÎµÎºÎ¹Î½Î¬Ï‰ Ï„Î·Î½ Ï€ÏÎ±ÎºÏ„Î¹ÎºÎ®",
                "keywords": ["Î¾ÎµÎºÎ¹Î½Î¬Ï‰", "Î¾ÎµÎºÎ¹Î½Ï", "Î±ÏÏ‡Î®", "Î±ÏÏ‡Î¯Î¶Ï‰", "Î±ÏÏ‡Î¯ÏƒÏ‰", "Î¾ÎµÎºÎ¯Î½Î·Î¼Î±", "Ï€ÏÎ±ÎºÏ„Î¹ÎºÎ®", "Î¬ÏƒÎºÎ·ÏƒÎ·", "Ï€ÏÏ‚", "Ï€Ï‰Ï‚", "Î²Î®Î¼Î±Ï„Î±", "Î´Î¹Î±Î´Î¹ÎºÎ±ÏƒÎ¯Î±", "Î´Î¹Î±Î´Î¹ÎºÎ±ÏƒÎ¹ÎµÏ‚"]
            },
            {
                "id": 2,
                "category": "ÎˆÎ³Î³ÏÎ±Ï†Î± & Î”Î¹Î±Î´Î¹ÎºÎ±ÏƒÎ¯ÎµÏ‚",
                "question": "Î¤Î¹ Î­Î³Î³ÏÎ±Ï†Î± Ï‡ÏÎµÎ¹Î¬Î¶Î¿Î¼Î±Î¹ Î³Î¹Î± Ï„Î·Î½ Ï€ÏÎ±ÎºÏ„Î¹ÎºÎ® Î¬ÏƒÎºÎ·ÏƒÎ·;",
                "answer": "Î“Î¹Î± ÎµÏƒÎ¬Ï‚ (Ï†Î¿Î¹Ï„Î·Ï„Î®):\nâ€¢ Î‘Î¯Ï„Î·ÏƒÎ· Ï€ÏÎ±Î³Î¼Î±Ï„Î¿Ï€Î¿Î¯Î·ÏƒÎ·Ï‚ Ï€ÏÎ±ÎºÏ„Î¹ÎºÎ®Ï‚ Î¬ÏƒÎºÎ·ÏƒÎ·Ï‚\nâ€¢ Î£Ï„Î¿Î¹Ï‡ÎµÎ¯Î± Ï†Î¿Î¹Ï„Î·Ï„Î® (ÏƒÏ…Î¼Ï€Î»Î·ÏÏ‰Î¼Î­Î½Î· Ï†ÏŒÏÎ¼Î±)\nâ€¢ Î‘ÏƒÏ†Î±Î»Î¹ÏƒÏ„Î¹ÎºÎ® Î¹ÎºÎ±Î½ÏŒÏ„Î·Ï„Î± Î±Ï€ÏŒ gov.gr\nâ€¢ Î¥Ï€ÎµÏÎ¸Ï…Î½Î· Î´Î®Î»Ï‰ÏƒÎ· (Î´ÎµÎ½ Ï€Î±Î¯ÏÎ½ÎµÏ„Îµ ÎµÏ€Î¯Î´Î¿Î¼Î± ÎŸÎ‘Î•Î”)\n\nÎ“Î¹Î± Ï„Î· Î´Î¿Î¼Î®:\nâ€¢ Î£Ï„Î¿Î¹Ï‡ÎµÎ¯Î± Ï†Î¿ÏÎ­Î± (Î‘Î¦Îœ, Î´Î¹ÎµÏÎ¸Ï…Î½ÏƒÎ·, Î½ÏŒÎ¼Î¹Î¼Î¿Ï‚ ÎµÎºÏ€ÏÏŒÏƒÏ‰Ï€Î¿Ï‚)\nâ€¢ Î—Î¼Î­ÏÎµÏ‚ ÎºÎ±Î¹ ÏÏÎµÏ‚ Ï€Î¿Ï… ÏƒÎ±Ï‚ Î´Î­Ï‡ÎµÏ„Î±Î¹\n\nTip: ÎÎµÎºÎ¹Î½Î®ÏƒÏ„Îµ Î±Ï€ÏŒ Ï„Î·Î½ Î±ÏƒÏ†Î±Î»Î¹ÏƒÏ„Î¹ÎºÎ® Î¹ÎºÎ±Î½ÏŒÏ„Î·Ï„Î± Î³Î¹Î±Ï„Î¯ Ï€Î±Î¯ÏÎ½ÎµÎ¹ Ï‡ÏÏŒÎ½Î¿!",
                "keywords": ["Î­Î³Î³ÏÎ±Ï†Î±", "ÎµÎ³Î³ÏÎ±Ï†Î±", "Ï‡Î±ÏÏ„Î¹Î¬", "Ï‡Î±ÏÏ„Î¹Î±", "Ï‡ÏÎµÎ¹Î¬Î¶Î¿Î¼Î±Î¹", "Ï‡ÏÎµÎ¹Î±Î¶Î¿Î¼Î±Î¹", "Î±Ï€Î±Î¹Ï„Î®ÏƒÎµÎ¹Ï‚", "Î±Ï€Î±Î¹Ï„Î·ÏƒÎµÎ¹Ï‚", "Î±Ï€Î±Î¹Ï„Î¿ÏÎ½Ï„Î±Î¹", "Î±Ï€Î±Î¹Ï„Î¿Ï…Î½Ï„Î±Î¹", "Î´Î¹ÎºÎ±Î¹Î¿Î»Î¿Î³Î·Ï„Î¹ÎºÎ¬", "Î´Î¹ÎºÎ±Î¹Î¿Î»Î¿Î³Î·Ï„Î¹ÎºÎ±", "Ï†Î¬ÎºÎµÎ»Î¿Ï‚", "Ï†Î±ÎºÎµÎ»Î¿Ï‚", "Î±Î¯Ï„Î·ÏƒÎ·", "Î±Î¹Ï„Î·ÏƒÎ·"]
            },
            {
                "id": 5,
                "category": "Î”Î¿Î¼Î­Ï‚ & Î¦Î¿ÏÎµÎ¯Ï‚",
                "question": "Î£Îµ Ï€Î¿Î¹ÎµÏ‚ Î´Î¿Î¼Î­Ï‚ Î¼Ï€Î¿ÏÏ Î½Î± ÎºÎ¬Î½Ï‰ Ï€ÏÎ±ÎºÏ„Î¹ÎºÎ® Î¬ÏƒÎºÎ·ÏƒÎ·;",
                "answer": "ÎœÏ€Î¿ÏÎµÎ¯Ï„Îµ Î½Î± ÎºÎ¬Î½ÎµÏ„Îµ Ï€ÏÎ±ÎºÏ„Î¹ÎºÎ® Î¬ÏƒÎºÎ·ÏƒÎ· ÏƒÎµ:\n\nâ€¢ Î‘Î¸Î»Î·Ï„Î¹ÎºÎ¿ÏÏ‚ ÏƒÏ…Î»Î»ÏŒÎ³Î¿Ï…Ï‚ (Ï€Î¿Î´ÏŒÏƒÏ†Î±Î¹ÏÎ¿, Î¼Ï€Î¬ÏƒÎºÎµÏ„, Î²ÏŒÎ»ÎµÏŠ, ÎµÎ½ÏŒÏÎ³Î±Î½Î· Î³Ï…Î¼Î½Î±ÏƒÏ„Î¹ÎºÎ®, ÎºÎ»Ï€)\nâ€¢ Î“Ï…Î¼Î½Î±ÏƒÏ„Î®ÏÎ¹Î± ÎºÎ±Î¹ fitness centers\nâ€¢ ÎšÎ¿Î»Ï…Î¼Î²Î·Ï„Î®ÏÎ¹Î±\nâ€¢ Î‘ÎºÎ±Î´Î·Î¼Î¯ÎµÏ‚ Î±Î¸Î»Î·Ï„Î¹ÏƒÎ¼Î¿Ï\nâ€¢ Î”Î·Î¼ÏŒÏƒÎ¹Î¿Ï…Ï‚ Î±Î¸Î»Î·Ï„Î¹ÎºÎ¿ÏÏ‚ Î¿ÏÎ³Î±Î½Î¹ÏƒÎ¼Î¿ÏÏ‚\nâ€¢ Î£Ï‡Î¿Î»ÎµÎ¯Î± (Î¼Îµ Ï„Î¼Î®Î¼Î± Ï†Ï…ÏƒÎ¹ÎºÎ®Ï‚ Î±Î³Ï‰Î³Î®Ï‚)\nâ€¢ ÎšÎ­Î½Ï„ÏÎ± Î±Ï€Î¿ÎºÎ±Ï„Î¬ÏƒÏ„Î±ÏƒÎ·Ï‚\nâ€¢ Personal training studios\n\nÎ— Î´Î¿Î¼Î® Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± Î­Ï‡ÎµÎ¹:\nâ€¢ Î•ÎºÏ€Î±Î¹Î´ÎµÏ…Ï„Î®/Ï…Ï€ÎµÏÎ¸Ï…Î½Î¿ Î¼Îµ Ï„Î± ÎºÎ±Ï„Î¬Î»Î»Î·Î»Î± Ï€ÏÎ¿ÏƒÏŒÎ½Ï„Î±\nâ€¢ ÎÏŒÎ¼Î¹Î¼Î· Î»ÎµÎ¹Ï„Î¿Ï…ÏÎ³Î¯Î± ÎºÎ±Î¹ Î‘Î¦Îœ\nâ€¢ Î”Ï…Î½Î±Ï„ÏŒÏ„Î·Ï„Î± Î½Î± ÏƒÎ±Ï‚ ÎºÎ±Î¸Î¿Î´Î·Î³Î®ÏƒÎµÎ¹ ÏƒÏ„Î·Î½ Ï€ÏÎ±ÎºÏ„Î¹ÎºÎ®",
                "keywords": ["Î´Î¿Î¼Î­Ï‚", "Î´Î¿Î¼Î·", "Ï†Î¿ÏÎµÎ¯Ï‚", "Ï†Î¿ÏÎµÎ¹Ï‚", "ÏƒÏÎ»Î»Î¿Î³Î¿Ï‚", "ÏƒÏ…Î»Î»Î¿Î³Î¿Ï‚", "Î³Ï…Î¼Î½Î±ÏƒÏ„Î®ÏÎ¹Î¿", "Î³Ï…Î¼Î½Î±ÏƒÏ„Î·ÏÎ¹Î¿", "ÎµÎ½ÏŒÏÎ³Î±Î½Î·", "ÎµÎ½Î¿ÏÎ³Î±Î½Î·", "Ï€Î¿Î´ÏŒÏƒÏ†Î±Î¹ÏÎ¿", "Ï€Î¿Î´Î¿ÏƒÏ†Î±Î¹ÏÎ¿", "Î¼Ï€Î¬ÏƒÎºÎµÏ„", "Î¼Ï€Î±ÏƒÎºÎµÏ„", "ÎºÎ¿Î»Ï…Î¼Î²Î·Ï„Î®ÏÎ¹Î¿", "ÎºÎ¿Î»Ï…Î¼Î²Î·Ï„Î·ÏÎ¹Î¿", "Î±ÎºÎ±Î´Î·Î¼Î¯Î±", "Î±ÎºÎ±Î´Î·Î¼Î¹Î±", "fitness", "personal", "training", "Ï€Î¿Ï…", "Ï€Î¿Î¹ÎµÏ‚", "Ï€Î¿Î¹Î¿Ï…Ï‚", "Ï€Î¿Î¹Î±"]
            },
            {
                "id": 30,
                "category": "ÎŸÎ¹ÎºÎ¿Î½Î¿Î¼Î¹ÎºÎ¬ & Î‘Î¼Î¿Î¹Î²Î®",
                "question": "Î Î±Î¯ÏÎ½Ï‰ Î±Î¼Î¿Î¹Î²Î® Î³Î¹Î± Ï„Î·Î½ Ï€ÏÎ±ÎºÏ„Î¹ÎºÎ® Î¬ÏƒÎºÎ·ÏƒÎ·; Î¤Î¹ ÎºÏŒÏƒÏ„Î¿Ï‚ Î­Ï‡ÎµÎ¹ Î³Î¹Î± Ï„Î· Î´Î¿Î¼Î®;",
                "answer": "Î“Î™Î‘ Î¤ÎŸÎ¥Î£ Î¦ÎŸÎ™Î¤Î—Î¤Î•Î£:\n\nÎ”Î•Î Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ Î±Î¼Î¿Î¹Î²Î® Î³Î¹Î± Ï„Î·Î½ Ï€ÏÎ±ÎºÏ„Î¹ÎºÎ® Î¬ÏƒÎºÎ·ÏƒÎ·\nâ€¢ Î— Ï€ÏÎ±ÎºÏ„Î¹ÎºÎ® Î¬ÏƒÎºÎ·ÏƒÎ· ÎµÎ¯Î½Î±Î¹ Î¼Î· Î±Î¼ÎµÎ¹Î²ÏŒÎ¼ÎµÎ½Î·\nâ€¢ Î•Î¯Î½Î±Î¹ Î¼Î­ÏÎ¿Ï‚ Ï„Ï‰Î½ ÏƒÏ€Î¿Ï…Î´ÏÎ½ ÏƒÎ±Ï‚\nâ€¢ Î”ÎµÎ½ Ï€ÏÏŒÎºÎµÎ¹Ï„Î±Î¹ Î³Î¹Î± ÎµÏÎ³Î±ÏƒÎ¹Î±ÎºÎ® ÏƒÏ‡Î­ÏƒÎ·\n\nÎ“Î™Î‘ Î¤Î— Î”ÎŸÎœÎ—:\n\nÎ— Î´Î¿Î¼Î® Î´Îµ Ï‡ÏÎµÏÎ½ÎµÏ„Î±Î¹ ÎºÎ¬Ï„Î¹ (ÏƒÏ‡ÎµÎ´ÏŒÎ½)\nâ€¢ Î¥Ï€Î¬ÏÏ‡ÎµÎ¹ Î­Î½Î± ÎµÎ»Î¬Ï‡Î¹ÏƒÏ„Î¿ Ï„Î­Î»Î¿Ï‚ Ï€Î¿Ï… ÎµÎ½Î´ÎµÏ‡Î¿Î¼Î­Î½Ï‰Ï‚ Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± ÎºÎ±Ï„Î±Î²Î¬Î»ÎµÎ¹\nâ€¢ Î¤Î¿ ÎºÎ¿Î»Î»Î­Î³Î¹Î¿ ÎºÎ±Î»ÏÏ€Ï„ÎµÎ¹ Ï„Î± Î­Î¾Î¿Î´Î± Ï„Î·Ï‚ ÏƒÏÎ¼Î²Î±ÏƒÎ·Ï‚\nâ€¢ Î— Î±ÏƒÏ†Î¬Î»Î¹ÏƒÎ· Ï„Î¹Î¼Î¿Î»Î¿Î³ÎµÎ¯Ï„Î±Î¹ ÏƒÏ„Î¿ ÎºÎ¿Î»Î»Î­Î³Î¹Î¿\nâ€¢ Î”ÎµÎ½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ Î¿Î¹ÎºÎ¿Î½Î¿Î¼Î¹ÎºÎ® Ï…Ï€Î¿Ï‡ÏÎ­Ï‰ÏƒÎ· Ï€ÏÎ¿Ï‚ Ï„Î¿Ï…Ï‚ Ï†Î¿Î¹Ï„Î·Ï„Î­Ï‚",
                "keywords": ["Î±Î¼Î¿Î¹Î²Î®", "Î±Î¼Î¿Î¹Î²Î·", "Ï€Î»Î·ÏÏ‰Î¼Î®", "Ï€Î»Î·ÏÏ‰Î¼Î·", "Ï€Î»Î·ÏÏÎ¸Ï", "Ï€Î»Î·ÏÏ‰Î¸Ï", "Ï€Î»Î·ÏÏ‰Î¸Ï‰", "Ï€Î»Î·ÏÏ‰Î½Î¿Î¼Î±Î¹", "Ï€Î»Î·ÏÏÎ½Î¿Î¼Î±Î¹", "Î»ÎµÏ†Ï„Î¬", "Î»ÎµÏ†Ï„Î±", "Ï‡ÏÎ®Î¼Î±Ï„Î±", "Ï‡ÏÎ·Î¼Î±Ï„Î±", "ÎºÏŒÏƒÏ„Î¿Ï‚", "ÎºÎ¿ÏƒÏ„Î¿Ï‚", "Ï„Î­Î»Î¿Ï‚", "Ï„ÎµÎ»Î¿Ï‚", "Î´Î¿Î¼Î®", "Î´Î¿Î¼Î·", "Ï†Î¿Î¹Ï„Î·Ï„Î®Ï‚", "Ï†Î¿Î¹Ï„Î·Ï„Î·", "Î¿Î¹ÎºÎ¿Î½Î¿Î¼Î¹ÎºÎ¬", "Î¿Î¹ÎºÎ¿Î½Î¿Î¼Î¹ÎºÎ±", "Î¼Î¹ÏƒÎ¸ÏŒÏ‚", "Î¼Î¹ÏƒÎ¸Î¿Ï‚"]
            },
            {
                "id": 11,
                "category": "Î•Ï€Î¹ÎºÎ¿Î¹Î½Ï‰Î½Î¯Î±",
                "question": "ÎœÎµ Ï€Î¿Î¹Î¿Î½ ÎµÏ€Î¹ÎºÎ¿Î¹Î½Ï‰Î½Ï Î³Î¹Î± Ï„Î·Î½ Ï€ÏÎ±ÎºÏ„Î¹ÎºÎ® Î¬ÏƒÎºÎ·ÏƒÎ·;",
                "answer": "ÎšÎ¥Î¡Î™Î‘ Î•Î Î™ÎšÎŸÎ™ÎÎ©ÎÎ™Î‘:\n\nÎ“ÎµÏÏÎ³Î¹Î¿Ï‚ Î£Î¿Ï†Î¹Î±Î½Î¯Î´Î·Ï‚, MSc, PhD(c)\nğŸ“§ gsofianidis@mitropolitiko.edu.gr\nÎ¥Ï€ÎµÏÎ¸Ï…Î½Î¿Ï‚ Î ÏÎ±ÎºÏ„Î¹ÎºÎ®Ï‚ Î†ÏƒÎºÎ·ÏƒÎ·Ï‚\n\nÎ•ÎÎ‘Î›Î›Î‘ÎšÎ¤Î™ÎšÎ— Î•Î Î™ÎšÎŸÎ™ÎÎ©ÎÎ™Î‘:\n\nÎ“ÎµÏÏÎ³Î¹Î¿Ï‚ ÎœÏ€Î¿Ï…Ï‡Î¿Ï…ÏÎ¬Ï‚, MSc, PhD\nğŸ“§ gbouchouras@mitropolitiko.edu.gr\nğŸ“ 2314 409000\nProgramme Leader\n\nÎ ÏŒÏ„Îµ Î½Î± ÎµÏ€Î¹ÎºÎ¿Î¹Î½Ï‰Î½Î®ÏƒÎµÏ„Îµ:\nâ€¢ Î•ÏÏ‰Ï„Î®ÏƒÎµÎ¹Ï‚ Î³Î¹Î± Î­Î³Î³ÏÎ±Ï†Î± âœ Î“ÎµÏÏÎ³Î¹Î¿Ï‚ Î£Î¿Ï†Î¹Î±Î½Î¯Î´Î·Ï‚\nâ€¢ Î¤ÎµÏ‡Î½Î¹ÎºÎ¬ Ï€ÏÎ¿Î²Î»Î®Î¼Î±Ï„Î± âœ Î“ÎµÏÏÎ³Î¹Î¿Ï‚ Î£Î¿Ï†Î¹Î±Î½Î¯Î´Î·Ï‚\nâ€¢ Î˜Î­Î¼Î±Ï„Î± Ï€ÏÎ¿Î³ÏÎ¬Î¼Î¼Î±Ï„Î¿Ï‚ âœ Î“ÎµÏÏÎ³Î¹Î¿Ï‚ ÎœÏ€Î¿Ï…Ï‡Î¿Ï…ÏÎ¬Ï‚",
                "keywords": ["ÎµÏ€Î¹ÎºÎ¿Î¹Î½Ï‰Î½Î¯Î±", "ÎµÏ€Î¹ÎºÎ¿Î¹Î½Ï‰Î½Î¹Î±", "Î£Î¿Ï†Î¹Î±Î½Î¯Î´Î·Ï‚", "Î£Î¿Ï†Î¹Î±Î½Î¹Î´Î·Ï‚", "ÎœÏ€Î¿Ï…Ï‡Î¿Ï…ÏÎ¬Ï‚", "ÎœÏ€Î¿Ï…Ï‡Î¿Ï…ÏÎ±Ï‚", "email", "Ï„Î·Î»Î­Ï†Ï‰Î½Î¿", "Ï„Î·Î»ÎµÏ†Ï‰Î½Î¿", "Ï…Ï€ÎµÏÎ¸Ï…Î½Î¿Ï‚", "Ï…Ï€ÎµÏ…Î¸Ï…Î½Î¿Ï‚", "Î²Î¿Î®Î¸ÎµÎ¹Î±", "Î²Î¿Î·Î¸ÎµÎ¹Î±", "ÎºÎ±Î¸Î·Î³Î·Ï„Î®Ï‚", "ÎºÎ±Î¸Î·Î³Î·Ï„Î·Ï‚", "ÎºÎ±Î¸Î·Î³Î®Ï„ÏÎ¹Î±", "ÎºÎ±Î¸Î·Î³Î·Ï„ÏÎ¹Î±", "contact", "ÏƒÏ„Î¿Î¹Ï‡ÎµÎ¯Î±", "ÏƒÏ„Î¿Î¹Ï‡ÎµÎ¹Î±"]
            },
            {
                "id": 4,
                "category": "ÎÏÎµÏ‚ & Î§ÏÎ¿Î½Î¿Î´Î¹Î¬Î³ÏÎ±Î¼Î¼Î±",
                "question": "Î ÏŒÏƒÎµÏ‚ ÏÏÎµÏ‚ Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± ÎºÎ¬Î½Ï‰ Ï€ÏÎ±ÎºÏ„Î¹ÎºÎ® Î¬ÏƒÎºÎ·ÏƒÎ·;",
                "answer": "Î¥Ï€Î¿Ï‡ÏÎµÏ‰Ï„Î¹ÎºÏŒ: Î¤Î¿Ï…Î»Î¬Ï‡Î¹ÏƒÏ„Î¿Î½ 240 ÏÏÎµÏ‚\n\nDeadline: ÎœÎ­Ï‡ÏÎ¹ 30 ÎœÎ¬ÏŠÎ¿Ï…\n\nÎšÎ±Î½ÏŒÎ½ÎµÏ‚ Ï‰ÏÎ±ÏÎ¯Î¿Ï…:\nâ€¢ Î”ÎµÏ…Ï„Î­ÏÎ± Î­Ï‰Ï‚ Î£Î¬Î²Î²Î±Ï„Î¿ (ÎŒÎ§Î™ ÎšÏ…ÏÎ¹Î±ÎºÎ­Ï‚, 5Î¼Î­ÏÎµÏ‚/ÎµÎ²Î´)\nâ€¢ ÎœÎ­Ï‡ÏÎ¹ 8 ÏÏÎµÏ‚ Ï„Î·Î½ Î·Î¼Î­ÏÎ±\nâ€¢ Î¤Î¿ Ï‰ÏÎ¬ÏÎ¹Î¿ Î¿ÏÎ¯Î¶ÎµÏ„Î±Î¹ Î±Ï€ÏŒ Ï„Î· Î´Î¿Î¼Î® ÏƒÎµ ÏƒÏ…Î½ÎµÏÎ³Î±ÏƒÎ¯Î± Î¼Î±Î¶Î¯ ÏƒÎ±Ï‚\n\nÎ¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚: 240 ÏÏÎµÏ‚ = Ï€ÎµÏÎ¯Ï€Î¿Ï… 6 ÎµÎ²Î´Î¿Î¼Î¬Î´ÎµÏ‚ x 40 ÏÏÎµÏ‚ Î® 8 ÎµÎ²Î´Î¿Î¼Î¬Î´ÎµÏ‚ x 30 ÏÏÎµÏ‚",
                "keywords": ["ÏÏÎµÏ‚", "Ï‰ÏÎµÏ‚", "240", "Ï€Î¿ÏƒÎµÏ‚", "Ï€ÏŒÏƒÎµÏ‚", "Ï€Î¿ÏƒÎ±", "Ï€Î¿ÏƒÎ¬", "ÏƒÏ…Î½Î¿Î»Î¹ÎºÎ¬", "ÏƒÏ…Î½Î¿Î»Î¹ÎºÎ±", "ÏŒÎ»ÎµÏ‚", "Î¿Î»ÎµÏ‚", "Ï„ÎµÎ»Î¹ÎºÎ¬", "Ï„ÎµÎ»Î¹ÎºÎ±", "Ï‡ÏÎ¿Î½Î¿Î´Î¹Î¬Î³ÏÎ±Î¼Î¼Î±", "Ï‡ÏÎ¿Î½Î¿Î´Î¹Î±Î³ÏÎ±Î¼Î¼Î±", "Î´Î¹Î¬ÏÎºÎµÎ¹Î±", "Î´Î¹Î±ÏÎºÎµÎ¹Î±", "Ï‡ÏÏŒÎ½Î¿Ï‚", "Ï‡ÏÎ¿Î½Î¿Ï‚", "30/5", "deadline"]
            }
        ]

    def download_pdf_file(self, filename: str) -> str:
        """Download and extract text from PDF file from GitHub"""
        if not PDF_AVAILABLE:
            print(f"âš ï¸ No PDF library available, cannot process {filename}")
            return ""
        
        # Check cache first
        if filename in self.pdf_cache:
            print(f"ğŸ“‹ Using cached content for {filename}")
            return self.pdf_cache[filename]
        
        try:
            base_url = "https://raw.githubusercontent.com/GiorgosBouh/chatbot.placement/main/"
            url = base_url + filename
            
            print(f"ğŸ” Downloading {filename} from GitHub using {PDF_METHOD}...")
            
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            
            text_content = []
            
            if PDF_METHOD == "PyPDF2":
                pdf_reader = PyPDF2.PdfReader(io.BytesIO(response.content))
                for page_num, page in enumerate(pdf_reader.pages):
                    try:
                        page_text = page.extract_text()
                        if page_text.strip():
                            text_content.append(page_text.strip())
                    except Exception as e:
                        print(f"âš ï¸ Error extracting page {page_num}: {e}")
                
            elif PDF_METHOD == "PyMuPDF":
                pdf_document = fitz.open(stream=response.content, filetype="pdf")
                for page_num in range(pdf_document.page_count):
                    try:
                        page = pdf_document[page_num]
                        page_text = page.get_text()
                        if page_text.strip():
                            text_content.append(page_text.strip())
                    except Exception as e:
                        print(f"âš ï¸ Error extracting page {page_num}: {e}")
                pdf_document.close()
            
            full_text = "\n".join(text_content)
            self.pdf_cache[filename] = full_text
            
            print(f"âœ… Successfully processed {filename} ({len(full_text)} characters)")
            return full_text
            
        except Exception as e:
            print(f"âŒ Failed to process {filename}: {e}")
            return ""

    def chunk_text(self, text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
        """Split text into overlapping chunks for better RAG performance"""
        if len(text) <= chunk_size:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + chunk_size
            
            # Try to break at sentence boundary
            if end < len(text):
                # Look for sentence ending within the last 100 characters
                search_start = max(start + chunk_size - 100, start)
                sentence_end = -1
                
                for delimiter in ['. ', '.\n', '! ', '!\n', '? ', '?\n']:
                    pos = text.rfind(delimiter, search_start, end)
                    if pos > sentence_end:
                        sentence_end = pos + len(delimiter)
                
                if sentence_end > start:
                    end = sentence_end
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            start = end - overlap
        
        return chunks

    def build_rag_database(self):
        """Build RAG vector database from Q&A and PDF content"""
        if not self.rag_initialized:
            print("âš ï¸ RAG not initialized, skipping database build")
            return
        
        print("ğŸ”„ Building RAG vector database...")
        
        self.document_chunks = []
        all_embeddings = []
        
        # Process Q&A data
        print("ğŸ“‹ Processing Q&A data for RAG...")
        for qa in self.qa_data:
            # Create chunks for question and answer separately
            qa_text = f"Î•ÏÏÏ„Î·ÏƒÎ·: {qa['question']} Î‘Ï€Î¬Î½Ï„Î·ÏƒÎ·: {qa['answer']}"
            chunks = self.chunk_text(qa_text, chunk_size=400, overlap=50)
            
            for i, chunk in enumerate(chunks):
                chunk_id = f"qa_{qa['id']}_{i}"
                doc_chunk = DocumentChunk(
                    id=chunk_id,
                    content=chunk,
                    source=f"Q&A Entry {qa['id']}",
                    chunk_type="qa",
                    metadata={
                        "category": qa.get('category', 'Unknown'),
                        "keywords": qa.get('keywords', []),
                        "qa_id": qa['id']
                    }
                )
                self.document_chunks.append(doc_chunk)
        
        # Process PDF content
        if PDF_AVAILABLE:
            print("ğŸ“„ Processing PDF files for RAG...")
            for filename in self.pdf_files:
                content = self.download_pdf_file(filename)
                if content:
                    chunks = self.chunk_text(content, chunk_size=600, overlap=100)
                    
                    for i, chunk in enumerate(chunks):
                        chunk_id = f"pdf_{filename}_{i}"
                        doc_chunk = DocumentChunk(
                            id=chunk_id,
                            content=chunk,
                            source=filename,
                            chunk_type="pdf",
                            metadata={
                                "filename": filename,
                                "chunk_index": i
                            }
                        )
                        self.document_chunks.append(doc_chunk)
        
        print(f"ğŸ“Š Created {len(self.document_chunks)} document chunks")
        
        # Generate embeddings
        if self.document_chunks:
            print("ğŸ§® Generating embeddings...")
            chunk_texts = [chunk.content for chunk in self.document_chunks]
            
            try:
                # Generate embeddings in batches to avoid memory issues
                batch_size = 32
                all_embeddings = []
                
                for i in range(0, len(chunk_texts), batch_size):
                    batch = chunk_texts[i:i + batch_size]
                    batch_embeddings = self.embedder.encode(batch, show_progress_bar=False)
                    all_embeddings.extend(batch_embeddings)
                
                # Create FAISS index
                embeddings_array = np.array(all_embeddings).astype('float32')
                
                # Normalize embeddings for cosine similarity
                faiss.normalize_L2(embeddings_array)
                
                # Use IndexFlatIP for inner product (cosine similarity with normalized vectors)
                self.faiss_index = faiss.IndexFlatIP(embeddings_array.shape[1])
                self.faiss_index.add(embeddings_array)
                
                print(f"âœ… RAG database built successfully with {len(self.document_chunks)} chunks")
                
            except Exception as e:
                print(f"âŒ Error building RAG database: {e}")
                self.faiss_index = None
        else:
            print("âš ï¸ No content available for RAG database")

    def retrieve_relevant_chunks(self, query: str, k: int = 5) -> List[Tuple[DocumentChunk, float]]:
        """Retrieve most relevant document chunks using RAG"""
        if not self.rag_initialized or self.faiss_index is None:
            print("âš ï¸ RAG not available for retrieval")
            return []
        
        try:
            # Encode query
            query_embedding = self.embedder.encode([query])
            query_embedding = query_embedding.astype('float32')
            faiss.normalize_L2(query_embedding)
            
            # Search for similar chunks
            scores, indices = self.faiss_index.search(query_embedding, k)
            
            relevant_chunks = []
            for score, idx in zip(scores[0], indices[0]):
                if idx < len(self.document_chunks):
                    chunk = self.document_chunks[idx]
                    relevant_chunks.append((chunk, float(score)))
            
            print(f"ğŸ” Retrieved {len(relevant_chunks)} relevant chunks (scores: {[f'{s:.3f}' for _, s in relevant_chunks]})")
            return relevant_chunks
            
        except Exception as e:
            print(f"âŒ Error in RAG retrieval: {e}")
            return []

    def get_rag_response(self, user_message: str) -> Tuple[str, bool]:
        """Get response using RAG (Retrieval-Augmented Generation)"""
        if not self.groq_client:
            return "", False
        
        print(f"ğŸ¤– Processing with RAG: '{user_message}'")
        
        try:
            # Retrieve relevant chunks
            relevant_chunks = self.retrieve_relevant_chunks(user_message, k=8)
            
            if not relevant_chunks:
                print("âš ï¸ No relevant chunks found, falling back to general knowledge")
                return self.get_fallback_ai_response(user_message)
            
            # Build context from retrieved chunks
            context_parts = []
            
            # Separate PDF and Q&A content
            pdf_chunks = [(chunk, score) for chunk, score in relevant_chunks if chunk.chunk_type == "pdf"]
            qa_chunks = [(chunk, score) for chunk, score in relevant_chunks if chunk.chunk_type == "qa"]
            
            # Add PDF context (official documents)
            if pdf_chunks:
                pdf_context = "\n\n".join([
                    f"[Î•Ï€Î¯ÏƒÎ·Î¼Î¿ Î­Î³Î³ÏÎ±Ï†Î¿: {chunk.source}]\n{chunk.content}"
                    for chunk, score in pdf_chunks[:4]  # Top 4 PDF chunks
                ])
                context_parts.append(f"Î•Î Î™Î£Î—ÎœÎ‘ Î•Î“Î“Î¡Î‘Î¦Î‘ ÎšÎŸÎ›Î›Î•Î“Î™ÎŸÎ¥:\n{pdf_context}")
            
            # Add Q&A context
            if qa_chunks:
                qa_context = "\n\n".join([
                    f"[ÎšÎ±Ï„Î·Î³Î¿ÏÎ¯Î±: {chunk.metadata.get('category', 'Î†Î»Î»Î±')}]\n{chunk.content}"
                    for chunk, score in qa_chunks[:4]  # Top 4 Q&A chunks
                ])
                context_parts.append(f"Î’Î‘Î£Î— Î“ÎÎ©Î£Î—Î£ Q&A:\n{qa_context}")
            
            # Build comprehensive prompt
            combined_context = "\n\n" + ("="*50 + "\n\n").join(context_parts)
            
            full_prompt = f"""Î‘ÎÎ‘ÎšÎ¤Î—ÎœÎ•ÎÎŸ Î Î•Î¡Î™Î•Î§ÎŸÎœÎ•ÎÎŸ Î‘Î ÎŸ Î£Î¥Î£Î¤Î—ÎœÎ‘ RAG:
{combined_context}

Î•Î¡Î©Î¤Î—Î£Î— Î¦ÎŸÎ™Î¤Î—Î¤Î—: {user_message}

ÎŸÎ”Î—Î“Î™Î•Î£ RAG:
1. Î‘Î½Î±Î»ÏÏƒÎµ Ï„Î¿ Î±Î½Î±ÎºÏ„Î·Î¼Î­Î½Î¿ Ï€ÎµÏÎ¹ÎµÏ‡ÏŒÎ¼ÎµÎ½Î¿ Î³Î¹Î± ÏƒÏ‡ÎµÏ„Î¹ÎºÏŒÏ„Î·Ï„Î± Î¼Îµ Ï„Î·Î½ ÎµÏÏÏ„Î·ÏƒÎ·
2. Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¯Î·ÏƒÎµ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚ Î±Ï€ÏŒ Î•Î Î™Î£Î—ÎœÎ‘ Î•Î“Î“Î¡Î‘Î¦Î‘ Ï‰Ï‚ ÎºÏÏÎ¹Î± Ï€Î·Î³Î®
3. Î£Ï…Î¼Ï€Î»Î®ÏÏ‰ÏƒÎµ Î¼Îµ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚ Î±Ï€ÏŒ Ï„Î· Î’Î‘Î£Î— Î“ÎÎ©Î£Î—Î£ Q&A
4. Î£Ï…Î½Î´ÏÎ±ÏƒÎµ Ï„Î¹Ï‚ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚ Î³Î¹Î± Î½Î± Î´ÏÏƒÎµÎ¹Ï‚ Î¼Î¹Î± Î¿Î»Î¿ÎºÎ»Î·ÏÏ‰Î¼Î­Î½Î· Î±Ï€Î¬Î½Ï„Î·ÏƒÎ·
5. Î•ÏƒÏ„Î¯Î±ÏƒÎµ ÏƒÏ„Î¹Ï‚ Ï€ÏÎ±ÎºÏ„Î¹ÎºÎ­Ï‚ ÏƒÏ…Î¼Î²Î¿Ï…Î»Î­Ï‚ ÎºÎ±Î¹ ÏƒÏ…Î³ÎºÎµÎºÏÎ¹Î¼Î­Î½Î± Î²Î®Î¼Î±Ï„Î±
6. Î‘Î½ Ï‡ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹ ÎµÏ€Î¹Î²ÎµÎ²Î±Î¯Ï‰ÏƒÎ·, Î±Î½Î±Ï†Î­ÏÎ¿Ï… Ï„Î¿Î½ Ï…Ï€ÎµÏÎ¸Ï…Î½Î¿

Î£Î¤Î¡Î‘Î¤Î—Î“Î™ÎšÎ— Î‘Î Î‘ÎÎ¤Î—Î£Î—Î£:
- Î”ÏÏƒÎµ Î¬Î¼ÎµÏƒÎ· ÎºÎ±Î¹ Ï‡ÏÎ®ÏƒÎ¹Î¼Î· Î±Ï€Î¬Î½Ï„Î·ÏƒÎ· Î²Î±ÏƒÎ¹ÏƒÎ¼Î­Î½Î· ÏƒÏ„Î¿ Î±Î½Î±ÎºÏ„Î·Î¼Î­Î½Î¿ Ï€ÎµÏÎ¹ÎµÏ‡ÏŒÎ¼ÎµÎ½Î¿
- Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¯Î·ÏƒÎµ Î´Î¿Î¼Î·Î¼Î­Î½Î· Ï€Î±ÏÎ¿Ï…ÏƒÎ¯Î±ÏƒÎ· Î¼Îµ ÏƒÎ±Ï†Î® Î²Î®Î¼Î±Ï„Î±
- Î£Ï…Î¼Ï€ÎµÏÎ¹Î­Î»Î±Î²Îµ ÏƒÏ…Î³ÎºÎµÎºÏÎ¹Î¼Î­Î½ÎµÏ‚ Î¿Î´Î·Î³Î¯ÎµÏ‚ ÎºÎ±Î¹ Ï€ÏÎ±ÎºÏ„Î¹ÎºÎ­Ï‚ ÏƒÏ…Î¼Î²Î¿Ï…Î»Î­Ï‚
- Î‘Î½Î±Ï†Î­ÏÎ¿Ï… ÏƒÏ‡ÎµÏ„Î¹ÎºÎ­Ï‚ Ï€ÏÎ¿Î¸ÎµÏƒÎ¼Î¯ÎµÏ‚ Î® Î±Ï€Î±Î¹Ï„Î®ÏƒÎµÎ¹Ï‚

Î‘Ï€Î¬Î½Ï„Î·ÏƒÎµ ÏƒÏ„Î± ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬ Î¼Îµ ÎµÏ€Î±Î³Î³ÎµÎ»Î¼Î±Ï„Î¹ÎºÏŒ Ï„ÏŒÎ½Î¿."""

            # Call Groq API
            chat_completion = self.groq_client.chat.completions.create(
                messages=[
                    {"role": "system", "content": self.system_prompt},
                    {"role": "user", "content": full_prompt}
                ],
                model="llama-3.1-8b-instant",
                temperature=0.2,  # Lower temperature for more focused responses
                max_tokens=1200,
                top_p=0.9,
                stream=False
            )

            response = chat_completion.choices[0].message.content
            
            # Validate Greek characters
            if response and any(ord(char) > 1500 and ord(char) not in range(0x0370, 0x03FF) for char in response):
                print("âš ï¸ Detected non-Greek characters in RAG response")
                return "", False
            
            print("âœ… RAG response generated successfully")
            return response, True
            
        except Exception as e:
            print(f"âŒ RAG Error: {e}")
            return "", False

    def get_fallback_ai_response(self, user_message: str) -> Tuple[str, bool]:
        """Fallback AI response when RAG is not available"""
        if not self.groq_client:
            return "", False
        
        try:
            fallback_prompt = f"""Î•Î¡Î©Î¤Î—Î£Î— Î¦ÎŸÎ™Î¤Î—Î¤Î—: {user_message}

Î Î›Î‘Î™Î£Î™ÎŸ: Î¦Î¿Î¹Ï„Î·Ï„Î®Ï‚ Î ÏÎ¿Ï€Î¿Î½Î·Ï„Î¹ÎºÎ®Ï‚ & Î¦Ï…ÏƒÎ¹ÎºÎ®Ï‚ Î‘Î³Ï‰Î³Î®Ï‚, ÎœÎ·Ï„ÏÎ¿Ï€Î¿Î»Î¹Ï„Î¹ÎºÏŒ ÎšÎ¿Î»Î»Î­Î³Î¹Î¿ Î˜ÎµÏƒÏƒÎ±Î»Î¿Î½Î¯ÎºÎ·Ï‚

Î’Î‘Î£Î™ÎšÎ•Î£ Î Î›Î—Î¡ÎŸÎ¦ÎŸÎ¡Î™Î•Î£:
- Î‘Ï€Î±Î¹Ï„Î¿ÏÎ½Ï„Î±Î¹ 240 ÏÏÎµÏ‚ Ï€ÏÎ±ÎºÏ„Î¹ÎºÎ®Ï‚ Î¬ÏƒÎºÎ·ÏƒÎ·Ï‚ Î¼Î­Ï‡ÏÎ¹ 30 ÎœÎ±ÎÎ¿Ï…
- Î”ÎµÏ…Ï„Î­ÏÎ±-Î£Î¬Î²Î²Î±Ï„Î¿, Î¼Î­Ï‡ÏÎ¹ 8 ÏÏÎµÏ‚/Î·Î¼Î­ÏÎ±  
- Î¥Ï€ÎµÏÎ¸Ï…Î½Î¿Ï‚: Î“ÎµÏÏÎ³Î¹Î¿Ï‚ Î£Î¿Ï†Î¹Î±Î½Î¯Î´Î·Ï‚ (gsofianidis@mitropolitiko.edu.gr)
- Î Î±ÏÎ¬Î´Î¿ÏƒÎ· ÏƒÏ…Î¼Î²Î¬ÏƒÎµÏ‰Î½ ÏƒÏ„Î¿ Moodle Î¼Î­Ï‡ÏÎ¹ 15 ÎŸÎºÏ„Ï‰Î²ÏÎ¯Î¿Ï…

ÎŸÎ”Î—Î“Î™Î•Î£:
1. Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¯Î·ÏƒÎµ Ï„Î· Î³ÎµÎ½Î¹ÎºÎ® ÏƒÎ¿Ï… Î³Î½ÏÏƒÎ· Î³Î¹Î± Ï€ÏÎ±ÎºÏ„Î¹ÎºÎ® Î¬ÏƒÎºÎ·ÏƒÎ· ÏƒÏ„Î·Î½ Î•Î»Î»Î¬Î´Î±
2. Î£Ï…ÏƒÏ‡Î­Ï„Î¹ÏƒÎµ Î¼Îµ Ï„Î¿ ÏƒÏ…Î³ÎºÎµÎºÏÎ¹Î¼Î­Î½Î¿ Ï€Î»Î±Î¯ÏƒÎ¹Î¿ Ï„Î¿Ï… ÎºÎ¿Î»Î»ÎµÎ³Î¯Î¿Ï…
3. Î”ÏÏƒÎµ Ï€ÏÎ±ÎºÏ„Î¹ÎºÎ­Ï‚ ÎºÎ±Î¹ Ï‡ÏÎ®ÏƒÎ¹Î¼ÎµÏ‚ ÏƒÏ…Î¼Î²Î¿Ï…Î»Î­Ï‚
4. Î ÏÏŒÏ„ÎµÎ¹Î½Îµ ÎµÏ€Î¹ÎºÎ¿Î¹Î½Ï‰Î½Î¯Î± Î¼Îµ Ï„Î¿Î½ Ï…Ï€ÎµÏÎ¸Ï…Î½Î¿ Î³Î¹Î± ÎµÏ€Î¹Î²ÎµÎ²Î±Î¯Ï‰ÏƒÎ·

Î‘Ï€Î¬Î½Ï„Î·ÏƒÎµ Î¼Îµ ÎµÏ€Î±Î³Î³ÎµÎ»Î¼Î±Ï„Î¹ÎºÏŒ Ï„ÏŒÎ½Î¿ ÏƒÏ„Î± ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬."""

            chat_completion = self.groq_client.chat.completions.create(
                messages=[
                    {"role": "system", "content": self.system_prompt},
                    {"role": "user", "content": fallback_prompt}
                ],
                model="llama-3.1-8b-instant",
                temperature=0.3,
                max_tokens=800,
                top_p=0.9,
                stream=False
            )

            response = chat_completion.choices[0].message.content
            
            if response and any(ord(char) > 1500 and ord(char) not in range(0x0370, 0x03FF) for char in response):
                print("âš ï¸ Detected non-Greek characters in fallback response")
                return "", False
            
            return response, True
            
        except Exception as e:
            print(f"âŒ Fallback AI Error: {e}")
            return "", False

    def get_smart_fallback_response(self, question: str) -> str:
        """Smart fallback response when AI is not available"""
        question_lower = question.lower()
        
        # Enhanced concept-based responses
        if any(keyword in question_lower for keyword in ['ÏƒÏÎ»Î»Î¿Î³Î¿', 'ÏƒÏÎ»Î»Î¿Î³Î¿Ï‚', 'Î³Ï…Î¼Î½Î±ÏƒÏ„Î®ÏÎ¹Î¿', 'Î´Î¿Î¼Î®', 'Ï†Î¿ÏÎ­Î±']):
            return """Î”ÎŸÎœÎ•Î£ Î Î¡Î‘ÎšÎ¤Î™ÎšÎ—Î£ Î‘Î£ÎšÎ—Î£Î—Î£:

â€¢ Î‘Î¸Î»Î·Ï„Î¹ÎºÎ¿ÏÏ‚ ÏƒÏ…Î»Î»ÏŒÎ³Î¿Ï…Ï‚ ÏŒÎ»Ï‰Î½ Ï„Ï‰Î½ Î±Î¸Î»Î·Î¼Î¬Ï„Ï‰Î½
â€¢ Î“Ï…Î¼Î½Î±ÏƒÏ„Î®ÏÎ¹Î± ÎºÎ±Î¹ fitness centers
â€¢ ÎšÎ¿Î»Ï…Î¼Î²Î·Ï„Î®ÏÎ¹Î±  
â€¢ Î‘ÎºÎ±Î´Î·Î¼Î¯ÎµÏ‚ Î±Î¸Î»Î·Ï„Î¹ÏƒÎ¼Î¿Ï
â€¢ Personal training studios
â€¢ ÎšÎ­Î½Ï„ÏÎ± Î±Ï€Î¿ÎºÎ±Ï„Î¬ÏƒÏ„Î±ÏƒÎ·Ï‚
â€¢ Î£Ï‡Î¿Î»ÎµÎ¯Î± Î¼Îµ Ï„Î¼Î®Î¼Î± Ï†Ï…ÏƒÎ¹ÎºÎ®Ï‚ Î±Î³Ï‰Î³Î®Ï‚

Î Î¡ÎŸÎ«Î ÎŸÎ˜Î•Î£Î•Î™Î£:
â€¢ ÎÏŒÎ¼Î¹Î¼Î· Î»ÎµÎ¹Ï„Î¿Ï…ÏÎ³Î¯Î± ÎºÎ±Î¹ Î‘Î¦Îœ
â€¢ Î•ÎºÏ€Î±Î¹Î´ÎµÏ…Ï„Î®Ï‚ Î¼Îµ ÎºÎ±Ï„Î¬Î»Î»Î·Î»Î± Ï€ÏÎ¿ÏƒÏŒÎ½Ï„Î±  
â€¢ Î”Ï…Î½Î±Ï„ÏŒÏ„Î·Ï„Î± ÎºÎ±Î¸Î¿Î´Î®Î³Î·ÏƒÎ·Ï‚

Î“Î¹Î± Î­Î³ÎºÏÎ¹ÏƒÎ· Î´Î¿Î¼Î®Ï‚: gsofianidis@mitropolitiko.edu.gr"""

        elif any(keyword in question_lower for keyword in ['Î­Î³Î³ÏÎ±Ï†Î±', 'Ï‡Î±ÏÏ„Î¹Î¬', 'Î´Î¹Î±Î´Î¹ÎºÎ±ÏƒÎ¯Î±', 'Î±Î¯Ï„Î·ÏƒÎ·']):
            return """Î‘Î Î‘Î™Î¤ÎŸÎ¥ÎœÎ•ÎÎ‘ Î•Î“Î“Î¡Î‘Î¦Î‘:

Î“Î™Î‘ Î¤ÎŸÎ Î¦ÎŸÎ™Î¤Î—Î¤Î—:
â€¢ Î‘Î¯Ï„Î·ÏƒÎ· Ï€ÏÎ±Î³Î¼Î±Ï„Î¿Ï€Î¿Î¯Î·ÏƒÎ·Ï‚ Ï€ÏÎ±ÎºÏ„Î¹ÎºÎ®Ï‚ Î¬ÏƒÎºÎ·ÏƒÎ·Ï‚
â€¢ Î£Ï„Î¿Î¹Ï‡ÎµÎ¯Î± Ï†Î¿Î¹Ï„Î·Ï„Î® (ÏƒÏ…Î¼Ï€Î»Î·ÏÏ‰Î¼Î­Î½Î· Ï†ÏŒÏÎ¼Î±)
â€¢ Î‘ÏƒÏ†Î±Î»Î¹ÏƒÏ„Î¹ÎºÎ® Î¹ÎºÎ±Î½ÏŒÏ„Î·Ï„Î± Î±Ï€ÏŒ gov.gr
â€¢ Î¥Ï€ÎµÏÎ¸Ï…Î½Î· Î´Î®Î»Ï‰ÏƒÎ· (Î¼Î· Î»Î®ÏˆÎ· ÎµÏ€Î¹Î´ÏŒÎ¼Î±Ï„Î¿Ï‚)

Î“Î™Î‘ Î¤Î— Î”ÎŸÎœÎ—:
â€¢ Î£Ï„Î¿Î¹Ï‡ÎµÎ¯Î± Ï†Î¿ÏÎ­Î± (Î‘Î¦Îœ, Î´Î¹ÎµÏÎ¸Ï…Î½ÏƒÎ·, ÎµÎºÏ€ÏÏŒÏƒÏ‰Ï€Î¿Ï‚)
â€¢ Î—Î¼Î­ÏÎµÏ‚ ÎºÎ±Î¹ ÏÏÎµÏ‚ Î´ÎµÎºÏ„ÏŒÏ„Î·Ï„Î±Ï‚

âš ï¸ Î£Î—ÎœÎ‘ÎÎ¤Î™ÎšÎŸ: ÎÎµÎºÎ¹Î½Î®ÏƒÏ„Îµ Î±Ï€ÏŒ Ï„Î·Î½ Î±ÏƒÏ†Î±Î»Î¹ÏƒÏ„Î¹ÎºÎ® Î¹ÎºÎ±Î½ÏŒÏ„Î·Ï„Î±!

Î•Ï€Î¹ÎºÎ¿Î¹Î½Ï‰Î½Î¯Î±: gsofianidis@mitropolitiko.edu.gr"""

        elif any(keyword in question_lower for keyword in ['ÏÏÎµÏ‚', 'Ï‡ÏÏŒÎ½Î¿Ï‚', 'Ï€ÏÎ¿Î¸ÎµÏƒÎ¼Î¯Î±', '240']):
            return """Î§Î¡ÎŸÎÎŸÎ”Î™Î‘Î“Î¡Î‘ÎœÎœÎ‘:

Î‘Ï€Î±Î¹Ï„Î¿ÏÎ¼ÎµÎ½ÎµÏ‚ ÏÏÎµÏ‚: 240 ÏÏÎµÏ‚
Î ÏÎ¿Î¸ÎµÏƒÎ¼Î¯Î±: 30 ÎœÎ±ÎÎ¿Ï…

ÎšÎ‘ÎÎŸÎÎ•Î£ Î©Î¡Î‘Î¡Î™ÎŸÎ¥:
â€¢ Î”ÎµÏ…Ï„Î­ÏÎ±-Î£Î¬Î²Î²Î±Ï„Î¿ (ÏŒÏ‡Î¹ ÎšÏ…ÏÎ¹Î±ÎºÎ­Ï‚)
â€¢ ÎœÎ­Ï‡ÏÎ¹ 8 ÏÏÎµÏ‚/Î·Î¼Î­ÏÎ±
â€¢ 5 Î·Î¼Î­ÏÎµÏ‚/ÎµÎ²Î´Î¿Î¼Î¬Î´Î±

Î Î‘Î¡Î‘Î”Î•Î™Î“ÎœÎ‘Î¤Î‘:
â€¢ 6 ÎµÎ²Î´Î¿Î¼Î¬Î´ÎµÏ‚ Ã— 40 ÏÏÎµÏ‚
â€¢ 8 ÎµÎ²Î´Î¿Î¼Î¬Î´ÎµÏ‚ Ã— 30 ÏÏÎµÏ‚  

Î“Î¹Î± Ï€ÏÎ¿ÏƒÎ±ÏÎ¼Î¿Î³Î®: gsofianidis@mitropolitiko.edu.gr"""

        else:
            return f"""Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ ÏƒÏ…Î³ÎºÎµÎºÏÎ¹Î¼Î­Î½Î· Î±Ï€Î¬Î½Ï„Î·ÏƒÎ·.

Î Î¡ÎŸÎ¤Î•Î™ÎÎŸÎœÎ•ÎÎ•Î£ Î•ÎÎ•Î¡Î“Î•Î™Î•Î£:
â€¢ Î”Î¹Î±Ï„Ï…Ï€ÏÏƒÏ„Îµ Ï„Î·Î½ ÎµÏÏÏ„Î·ÏƒÎ· Ï€Î¹Î¿ ÏƒÏ…Î³ÎºÎµÎºÏÎ¹Î¼Î­Î½Î±
â€¢ Î•Ï€Î¹Î»Î­Î¾Ï„Îµ Î±Ï€ÏŒ Ï„Î¹Ï‚ ÏƒÏ…Ï‡Î½Î­Ï‚ ÎµÏÏ‰Ï„Î®ÏƒÎµÎ¹Ï‚
â€¢ Î•Ï€Î¹ÎºÎ¿Î¹Î½Ï‰Î½Î®ÏƒÏ„Îµ Î¼Îµ Ï„Î¿Î½ Ï…Ï€ÎµÏÎ¸Ï…Î½Î¿

Î•Î Î™ÎšÎŸÎ™ÎÎ©ÎÎ™Î‘:
ğŸ“§ gsofianidis@mitropolitiko.edu.gr
ğŸ“ 2314 409000

Î“Î¹Î± Î¬Î¼ÎµÏƒÎ· Î²Î¿Î®Î¸ÎµÎ¹Î±, Ï€ÎµÏÎ¹Î³ÏÎ¬ÏˆÏ„Îµ Ï„Î· ÏƒÏ…Î³ÎºÎµÎºÏÎ¹Î¼Î­Î½Î· Î±Ï€Î¿ÏÎ¯Î±."""

    def get_response(self, question: str) -> str:
        """Main response method using RAG-first approach"""
        if not self.qa_data:
            return "Î”ÎµÎ½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ Î´Î¹Î±Î¸Î­ÏƒÎ¹Î¼Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î³Î½ÏÏƒÎ·Ï‚."
        
        print(f"\nğŸ¤– Processing question with RAG: '{question}'")
        
        # RAG-first approach
        if self.rag_initialized and self.faiss_index is not None:
            print("ğŸ§  Step 1: RAG semantic search...")
            response, success = self.get_rag_response(question)
            if success and response.strip():
                print("âœ… RAG response successful")
                return response
            else:
                print("âš ï¸ RAG failed, trying fallback AI...")
        else:
            print("âš ï¸ RAG not available, using fallback AI...")
        
        # Fallback to AI without RAG
        if self.groq_client:
            response, success = self.get_fallback_ai_response(question)
            if success and response.strip():
                print("âœ… Fallback AI response successful")
                return response
        
        # Final fallback to smart responses
        print("ğŸ“‹ Using smart fallback response...")
        return self.get_smart_fallback_response(question)

def main():
    """Main Streamlit application with RAG-powered intelligence"""
    
    # Enhanced Responsive CSS Styling
    st.markdown("""
    <style>
    /* Base responsive styles */
    .main-header {
        background: linear-gradient(90deg, #1f4e79 0%, #2980b9 100%);
        color: white;
        padding: 1.5rem;
        border-radius: 10px;
        text-align: center;
        margin-bottom: 2rem;
        box-shadow: 0 4px 12px rgba(0,0,0,0.15);
        display: flex;
        align-items: center;
        justify-content: center;
        gap: 2rem;
        flex-wrap: wrap;
    }
    
    .header-content {
        flex: 1;
        min-width: 300px;
    }
    
    .header-logo {
        max-height: 80px;
        max-width: 120px;
        object-fit: contain;
        flex-shrink: 0;
    }
    
    .user-message {
        background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
        border: 1px solid #dee2e6;
        border-left: 4px solid #007bff;
        padding: 1rem;
        border-radius: 8px;
        margin: 1rem 0;
        color: #333;
        word-wrap: break-word;
        overflow-wrap: break-word;
    }
    
    .ai-message {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        border: none;
        border-left: 4px solid #4caf50;
        padding: 1rem;
        border-radius: 8px;
        margin: 1rem 0;
        box-shadow: 0 4px 8px rgba(0,0,0,0.15);
        word-wrap: break-word;
        overflow-wrap: break-word;
    }
    
    .ai-message a {
        color: #ffeb3b !important;
        text-decoration: underline !important;
        font-weight: bold !important;
    }
    
    .ai-message a:hover {
        color: #fff9c4 !important;
        text-decoration: underline !important;
    }
    
    .info-card {
        background: white;
        border: 1px solid #e8f4f8;
        border-radius: 10px;
        padding: 1.5rem;
        margin: 0.5rem;
        box-shadow: 0 2px 8px rgba(0,0,0,0.08);
        transition: transform 0.2s ease, box-shadow 0.2s ease;
        min-height: 120px;
        display: flex;
        flex-direction: column;
        justify-content: center;
    }
    
    .info-card:hover {
        transform: translateY(-2px);
        box-shadow: 0 4px 16px rgba(0,0,0,0.12);
    }
    
    .api-status {
        position: fixed;
        top: 20px;
        right: 20px;
        background: #28a745;
        color: white;
        padding: 0.5rem 1rem;
        border-radius: 20px;
        font-size: 0.9rem;
        z-index: 1000;
        box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    
    .rag-status {
        background: linear-gradient(45deg, #ff6b6b, #4ecdc4);
        animation: gradientShift 3s ease-in-out infinite;
    }
    
    @keyframes gradientShift {
        0%, 100% { background: linear-gradient(45deg, #ff6b6b, #4ecdc4); }
        50% { background: linear-gradient(45deg, #4ecdc4, #45b7d1); }
    }
    
    .chat-container {
        background: white;
        border-radius: 10px;
        padding: 1rem;
        margin-top: 2rem;
        box-shadow: 0 2px 10px rgba(0,0,0,0.05);
        border: 1px solid #e8f4f8;
    }
    
    .stTextInput > div > div > input {
        border-radius: 20px;
        border: 2px solid #e8f4f8;
        padding: 0.8rem 1.2rem;
        font-size: 16px;
    }
    
    .stButton > button {
        background: linear-gradient(90deg, #1f4e79 0%, #2980b9 100%);
        color: white;
        border: none;
        border-radius: 20px;
        padding: 0.6rem 2rem;
        font-weight: 600;
        transition: all 0.3s ease;
        width: 100%;
        min-height: 44px;
    }
    
    .stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 4px 12px rgba(31, 78, 121, 0.3);
    }
    
    /* Responsive breakpoints */
    @media screen and (max-width: 768px) {
        .main-header {
            padding: 1rem;
            gap: 1rem;
            flex-direction: column;
            text-align: center;
        }
        
        .header-content h1 {
            font-size: 1.5rem !important;
            margin-bottom: 0.5rem;
        }
        
        .header-content h3 {
            font-size: 1rem !important;
            margin-bottom: 0.5rem;
        }
        
        .header-content p {
            font-size: 0.9rem !important;
        }
        
        .header-logo {
            max-height: 60px;
            max-width: 100px;
        }
        
        .info-card {
            margin: 0.25rem;
            padding: 1rem;
            min-height: auto;
        }
        
        .info-card h4 {
            font-size: 1rem !important;
        }
        
        .info-card p {
            font-size: 1rem !important;
        }
        
        .info-card small {
            font-size: 0.8rem !important;
        }
        
        .chat-container {
            padding: 0.5rem;
            margin-left: -1rem;
            margin-right: -1rem;
            border-radius: 0;
        }
        
        .user-message, .ai-message {
            padding: 0.75rem;
            font-size: 0.9rem;
        }
        
        .api-status {
            position: relative;
            top: auto;
            right: auto;
            margin: 1rem 0;
            display: inline-block;
            font-size: 0.8rem;
        }
    }
    
    @media screen and (max-width: 480px) {
        .main-header {
            padding: 0.75rem;
        }
        
        .header-content h1 {
            font-size: 1.25rem !important;
        }
        
        .header-content h3 {
            font-size: 0.9rem !important;
        }
        
        .info-card {
            padding: 0.75rem;
        }
        
        .info-card h4 {
            font-size: 0.9rem !important;
            margin-bottom: 0.25rem !important;
        }
        
        .info-card p {
            font-size: 0.9rem !important;
            margin: 0.25rem 0 !important;
        }
        
        .user-message, .ai-message {
            padding: 0.5rem;
            font-size: 0.85rem;
            margin: 0.5rem 0;
        }
        
        .stButton > button {
            padding: 0.5rem 1rem;
            font-size: 0.9rem;
        }
    }
    
    /* Hide scrollbars but keep functionality */
    .stApp {
        overflow-x: hidden;
    }
    
    /* Ensure content doesn't break on very small screens */
    * {
        max-width: 100%;
        box-sizing: border-box;
    }
    
    /* Better text scaling */
    html {
        -webkit-text-size-adjust: 100%;
        -ms-text-size-adjust: 100%;
    }
    </style>
    """, unsafe_allow_html=True)

    # Header with Logo
    logo_url = "https://raw.githubusercontent.com/GiorgosBouh/chatbot.placement/main/MK_LOGO_SEO_1200x630.png"
    
    st.markdown(f"""
    <div class="main-header">
        <img src="{logo_url}" alt="ÎœÎ·Ï„ÏÎ¿Ï€Î¿Î»Î¹Ï„Î¹ÎºÏŒ ÎšÎ¿Î»Î»Î­Î³Î¹Î¿" class="header-logo">
        <div class="header-content">
            <h1>Î ÏÎ±ÎºÏ„Î¹ÎºÎ® Î†ÏƒÎºÎ·ÏƒÎ·</h1>
            <h3>ÎœÎ·Ï„ÏÎ¿Ï€Î¿Î»Î¹Ï„Î¹ÎºÏŒ ÎšÎ¿Î»Î»Î­Î³Î¹Î¿ - Î¤Î¼Î®Î¼Î± Î ÏÎ¿Ï€Î¿Î½Î·Ï„Î¹ÎºÎ®Ï‚ & Î¦Ï…ÏƒÎ¹ÎºÎ®Ï‚ Î‘Î³Ï‰Î³Î®Ï‚</h3>
            <p><em>ğŸ§  RAG-Powered AI Assistant Î¼Îµ Î£Î·Î¼Î±ÏƒÎ¹Î¿Î»Î¿Î³Î¹ÎºÎ® Î‘Î½Î±Î¶Î®Ï„Î·ÏƒÎ·</em></p>
        </div>
    </div>
    """, unsafe_allow_html=True)

    # Initialize session state
    if 'messages' not in st.session_state:
        st.session_state.messages = []

    if 'chatbot' not in st.session_state:
        # Get Groq API key
        groq_api_key = None
        try:
            groq_api_key = st.secrets.get("GROQ_API_KEY") or os.environ.get("GROQ_API_KEY")
        except:
            pass
        
        with st.spinner("ğŸ”„ Initializing RAG system..."):
            st.session_state.chatbot = RAGInternshipChatbot(groq_api_key)
    else:
        # Refresh data if needed
        current_data_count = len(st.session_state.chatbot.qa_data)
        st.session_state.chatbot.qa_data = st.session_state.chatbot.load_qa_data()
        new_data_count = len(st.session_state.chatbot.qa_data)
        
        if new_data_count != current_data_count:
            st.toast(f"ğŸ“Š Data updated: {new_data_count} entries")
            # Rebuild RAG database if needed
            if st.session_state.chatbot.rag_initialized:
                with st.spinner("ğŸ”„ Rebuilding RAG database..."):
                    st.session_state.chatbot.build_rag_database()

    # Quick info cards
    st.markdown("### ğŸ“Š Î£Î·Î¼Î±Î½Ï„Î¹ÎºÎ­Ï‚ Î Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚")
    
    quick_col1, quick_col2, quick_col3 = st.columns([1, 1, 1])
    
    with quick_col1:
        st.markdown("""
        <div class="info-card" style="text-align: center;">
            <h4 style="color: #1f4e79; margin-bottom: 0.5rem;">ğŸ“… Î‘Ï€Î±Î¹Ï„Î¿ÏÎ¼ÎµÎ½ÎµÏ‚ ÎÏÎµÏ‚</h4>
            <p style="font-size: 1.2rem; font-weight: 600; color: #28a745; margin: 0;">240 ÏÏÎµÏ‚</p>
            <small style="color: #6c757d;">Î ÏÎ¿Î¸ÎµÏƒÎ¼Î¯Î±: 30 ÎœÎ±ÏŠÎ¿Ï…</small>
        </div>
        """, unsafe_allow_html=True)

    with quick_col2:
        st.markdown("""
        <div class="info-card" style="text-align: center;">
            <h4 style="color: #1f4e79; margin-bottom: 0.5rem;">ğŸ“‹ Î Î±ÏÎ¬Î´Î¿ÏƒÎ· Î£Ï…Î¼Î²Î¬ÏƒÎµÏ‰Î½</h4>
            <p style="font-size: 1.2rem; font-weight: 600; color: #ffc107; margin: 0;">Moodle Platform</p>
            <small style="color: #6c757d;">Î ÏÎ¿Î¸ÎµÏƒÎ¼Î¯Î±: 15 ÎŸÎºÏ„Ï‰Î²ÏÎ¯Î¿Ï…</small>
        </div>
        """, unsafe_allow_html=True)

    with quick_col3:
        st.markdown("""
        <div class="info-card" style="text-align: center;">
            <h4 style="color: #1f4e79; margin-bottom: 0.5rem;">â° Î•Ï€Î¹Ï„ÏÎµÏ€ÏŒÎ¼ÎµÎ½Î¿ Î©ÏÎ¬ÏÎ¹Î¿</h4>
            <p style="font-size: 1.2rem; font-weight: 600; color: #17a2b8; margin: 0;">Î”ÎµÏ…Ï„Î­ÏÎ±-Î£Î¬Î²Î²Î±Ï„Î¿</p>
            <small style="color: #6c757d;">ÎœÎ­Ï‡ÏÎ¹ 8 ÏÏÎµÏ‚ Î±Î½Î¬ Î·Î¼Î­ÏÎ±</small>
        </div>
        """, unsafe_allow_html=True)

    # RAG Status Indicator
    if st.session_state.chatbot.rag_initialized and st.session_state.chatbot.faiss_index is not None:
        chunks_count = len(st.session_state.chatbot.document_chunks)
        st.markdown(f'<div class="api-status rag-status">ğŸ§  RAG Active ({chunks_count} chunks)</div>', unsafe_allow_html=True)
    elif st.session_state.chatbot.groq_client:
        st.markdown('<div class="api-status">ğŸ¤– AI Mode</div>', unsafe_allow_html=True)
    else:
        st.markdown('<div class="api-status" style="background: #ffc107;">ğŸ“‹ Basic Mode</div>', unsafe_allow_html=True)

    # Enhanced status information
    if st.session_state.chatbot.rag_initialized:
        status_text = f"RAG Semantic Search â†’ AI Generation â†’ Smart Fallback ({len(st.session_state.chatbot.document_chunks)} chunks)"
    elif st.session_state.chatbot.groq_client:
        status_text = "AI Generation â†’ Smart Fallback"
    else:
        status_text = "Smart Concept-Based Responses"
    
    st.markdown(f"""
    <div style="background: #f8f9fa; border: 1px solid #dee2e6; border-radius: 4px; padding: 0.6rem; margin-bottom: 1.5rem; text-align: center; font-size: 0.9rem;">
        <strong>ğŸ§  RAG-Powered Assistant:</strong> Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯ ÏƒÎ·Î¼Î±ÏƒÎ¹Î¿Î»Î¿Î³Î¹ÎºÎ® Î±Î½Î±Î¶Î®Ï„Î·ÏƒÎ· Î³Î¹Î± Î²Î±Î¸ÏÏ„ÎµÏÎ· ÎºÎ±Ï„Î±Î½ÏŒÎ·ÏƒÎ·<br>
        <small>ğŸ”„ Architecture: {status_text}</small>
    </div>
    """, unsafe_allow_html=True)

    # Sidebar
    with st.sidebar:
        st.markdown("## ğŸ—£ï¸ Î•Ï€Î¹ÎºÎ¿Î¹Î½Ï‰Î½Î¯Î±")
        
        st.markdown("""
        **Î¥Ï€ÎµÏÎ¸Ï…Î½Î¿Ï‚ Î ÏÎ±ÎºÏ„Î¹ÎºÎ®Ï‚ Î†ÏƒÎºÎ·ÏƒÎ·Ï‚**  
        **Î“ÎµÏÏÎ³Î¹Î¿Ï‚ Î£Î¿Ï†Î¹Î±Î½Î¯Î´Î·Ï‚**  
        **ğŸ“ 2314409000**
        **ğŸ“§ gsofianidis@mitropolitiko.edu.gr**
        
        **Î£Ï‡ÎµÎ´Î¹Î±ÏƒÎ¼ÏŒÏ‚/Î‘Î½Î¬Ï€Ï„Ï…Î¾Î·/Î¤ÎµÏ‡Î½Î¹ÎºÎ® Î¥Ï€Î¿ÏƒÏ„Î®ÏÎ¹Î¾Î·**  
        **Î“ÎµÏÏÎ³Î¹Î¿Ï‚ ÎœÏ€Î¿Ï…Ï‡Î¿Ï…ÏÎ¬Ï‚**  
        ğŸ“§ gbouchouras@mitropolitiko.edu.gr

        âš ï¸ ÎšÎ±Î½Î­Î½Î± ÎµÏ€Î¯ÏƒÎ·Î¼Î¿ Î­Î³Î³ÏÎ±Ï†Î¿ Î´ÎµÎ½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ Î® ÎºÎ±Ï„Î±Ï„Î¯Î¸ÎµÏ„Î±Î¹ ÏƒÏ„Î·Î½ Ï€Î±ÏÎ¿ÏÏƒÎ± ÎµÏ†Î±ÏÎ¼Î¿Î³Î®
        """)

        st.markdown("---")

        # Frequent questions
        st.markdown("## ğŸ”„ Î£Ï…Ï‡Î½Î­Ï‚ Î•ÏÏ‰Ï„Î®ÏƒÎµÎ¹Ï‚")
        
        categories = {}
        for qa in st.session_state.chatbot.qa_data:
            cat = qa.get('category', 'Î†Î»Î»Î±')
            if cat not in categories:
                categories[cat] = []
            categories[cat].append(qa)

        for category, questions in categories.items():
            with st.expander(f"ğŸ“‚ {category}"):
                for qa in questions:
                    if st.button(qa['question'], key=f"faq_{qa['id']}", use_container_width=True):
                        st.session_state.messages.append({"role": "user", "content": qa['question']})
                        st.session_state.messages.append({"role": "assistant", "content": qa['answer']})
                        st.rerun()

        st.markdown("---")

        # Enhanced RAG Status
        if st.session_state.chatbot.rag_initialized:
            if st.session_state.chatbot.faiss_index is not None:
                st.success("ğŸ§  RAG System Active")
                chunks = len(st.session_state.chatbot.document_chunks)
                st.info(f"Semantic search across {chunks} document chunks")
                
                # RAG Statistics
                qa_chunks = sum(1 for chunk in st.session_state.chatbot.document_chunks if chunk.chunk_type == "qa")
                pdf_chunks = sum(1 for chunk in st.session_state.chatbot.document_chunks if chunk.chunk_type == "pdf")
                st.write(f"ğŸ“‹ Q&A chunks: {qa_chunks}")
                st.write(f"ğŸ“„ PDF chunks: {pdf_chunks}")
            else:
                st.warning("ğŸ§  RAG Initialized but Database Missing")
        else:
            if RAG_AVAILABLE:
                st.warning("ğŸ§  RAG Libraries Available but Not Initialized")
            else:
                st.error("âš ï¸ RAG Libraries Not Available")
                st.info("Install: pip install sentence-transformers faiss-cpu")

        st.markdown("---")

        if st.button("ğŸ—‘ï¸ ÎÎ­Î± Î£Ï…Î½Î¿Î¼Î¹Î»Î¯Î±", use_container_width=True):
            st.session_state.messages = []
            st.rerun()

        # Enhanced Technical Information
        with st.expander("ğŸ”§ RAG System Details"):
            st.markdown("**For technical issues:**")
            st.markdown("ğŸ“§ gbouchouras@mitropolitiko.edu.gr")
            
            st.write("**RAG System Status:**")
            st.write("â€¢ RAG Libraries:", RAG_AVAILABLE)
            st.write("â€¢ RAG Initialized:", st.session_state.chatbot.rag_initialized)
            st.write("â€¢ Vector Database:", st.session_state.chatbot.faiss_index is not None)
            st.write("â€¢ Embedding Model:", "paraphrase-multilingual-MiniLM-L12-v2" if st.session_state.chatbot.rag_initialized else "None")
            st.write("â€¢ Groq Available:", GROQ_AVAILABLE)
            st.write("â€¢ Groq Client:", st.session_state.chatbot.groq_client is not None)
            st.write("â€¢ PDF Available:", PDF_AVAILABLE)
            
            if st.session_state.chatbot.rag_initialized:
                st.write("**Document Chunks:**")
                st.write(f"â€¢ Total chunks: {len(st.session_state.chatbot.document_chunks)}")
                
                chunk_types = {}
                for chunk in st.session_state.chatbot.document_chunks:
                    chunk_types[chunk.chunk_type] = chunk_types.get(chunk.chunk_type, 0) + 1
                
                for chunk_type, count in chunk_types.items():
                    st.write(f"â€¢ {chunk_type.upper()} chunks: {count}")
                
                # RAG Test
                st.subheader("ğŸ§  RAG Retrieval Test")
                test_query = st.text_input("Test RAG query:", placeholder="Î¤Î¹ Î­Î³Î³ÏÎ±Ï†Î± Ï‡ÏÎµÎ¹Î¬Î¶Î¿Î¼Î±Î¹;")
                if test_query:
                    relevant_chunks = st.session_state.chatbot.retrieve_relevant_chunks(test_query, k=3)
                    if relevant_chunks:
                        st.write("**Retrieved chunks:**")
                        for i, (chunk, score) in enumerate(relevant_chunks):
                            st.write(f"**Chunk {i+1}** (score: {score:.3f}) - {chunk.source}")
                            st.write(f"Type: {chunk.chunk_type}")
                            st.write(f"Content preview: {chunk.content[:200]}...")
                            st.markdown("---")
                    else:
                        st.write("No relevant chunks found")
            
            # Enhanced file status
            qa_file_exists = os.path.exists("qa_data.json")
            st.write("**Data Sources:**")
            st.write("â€¢ qa_data.json exists:", qa_file_exists)
            st.write("â€¢ QA Data Count:", len(st.session_state.chatbot.qa_data))
            
            if PDF_AVAILABLE:
                st.write("â€¢ PDF Files:", len(st.session_state.chatbot.pdf_files))
                cached_pdfs = len(st.session_state.chatbot.pdf_cache)
                st.write(f"â€¢ Cached PDFs: {cached_pdfs}/{len(st.session_state.chatbot.pdf_files)}")

    # Chat interface
    st.markdown('<div class="chat-container">', unsafe_allow_html=True)
    st.markdown("### ğŸ’¬ ÎšÎ¬Î½Ï„Îµ Ï„Î·Î½ ÎµÏÏÏ„Î·ÏƒÎ® ÏƒÎ±Ï‚")

    # Display chat messages
    for message in st.session_state.messages:
        if message["role"] == "user":
            st.markdown(f'<div class="user-message"><strong>Î•ÏƒÎµÎ¯Ï‚:</strong> {message["content"]}</div>', unsafe_allow_html=True)
        else:
            content = message["content"].replace('\n', '<br>')
            if st.session_state.chatbot.rag_initialized:
                assistant_name = "ğŸ§  RAG Assistant"
            else:
                assistant_name = "ğŸ¤– Smart Assistant"
            st.markdown(f'<div class="ai-message"><strong>{assistant_name}:</strong><br><br>{content}</div>', unsafe_allow_html=True)

    st.markdown('</div>', unsafe_allow_html=True)

    # Chat input
    user_input = st.chat_input("Î“ÏÎ¬ÏˆÏ„Îµ Ï„Î·Î½ ÎµÏÏÏ„Î·ÏƒÎ® ÏƒÎ±Ï‚ ÎµÎ´Ï...")
    
    if user_input:
        st.session_state.messages.append({"role": "user", "content": user_input})
        
        spinner_text = "Performing semantic search and generating response..." if st.session_state.chatbot.rag_initialized else "Generating intelligent response..."
        
        with st.spinner(spinner_text):
            try:
                response = st.session_state.chatbot.get_response(user_input)
            except Exception as e:
                response = f"Î£Ï…Î³Î³Î½ÏÎ¼Î·, Ï€Î±ÏÎ¿Ï…ÏƒÎ¹Î¬ÏƒÏ„Î·ÎºÎµ ÏƒÏ†Î¬Î»Î¼Î±: {str(e)}"
                st.error(f"Error: {e}")
        
        st.session_state.messages.append({"role": "assistant", "content": response})
        st.rerun()

    # Footer
    if st.session_state.chatbot.rag_initialized:
        footer_text = "RAG-Powered Semantic Search Assistant"
    elif st.session_state.chatbot.groq_client:
        footer_text = "AI-Enhanced Smart Assistant"
    else:
        footer_text = "Concept-Based Smart Assistant"
    
    st.markdown(f"""
    <div style="text-align: center; color: #6c757d; padding: 1rem; font-size: 0.9rem;">
        <small>
            ğŸ“ <strong>ÎœÎ·Ï„ÏÎ¿Ï€Î¿Î»Î¹Ï„Î¹ÎºÏŒ ÎšÎ¿Î»Î»Î­Î³Î¹Î¿ Î˜ÎµÏƒÏƒÎ±Î»Î¿Î½Î¯ÎºÎ·Ï‚</strong> | 
            Î¤Î¼Î®Î¼Î± Î ÏÎ¿Ï€Î¿Î½Î·Ï„Î¹ÎºÎ®Ï‚ & Î¦Ï…ÏƒÎ¹ÎºÎ®Ï‚ Î‘Î³Ï‰Î³Î®Ï‚<br>
            <em>{footer_text}</em>
        </small>
    </div>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()